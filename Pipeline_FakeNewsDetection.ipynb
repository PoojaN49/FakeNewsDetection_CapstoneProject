{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pipeline_FakeNewsDetection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7PBQqvYR8SB"
      },
      "source": [
        "**Installing all the required Librairies**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wFomeKKqRhrl",
        "outputId": "123f950d-5aae-45c4-f2ca-1780cbc20289"
      },
      "source": [
        "!pip install streamlit    #### installing stream lit for deployment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/11/57097e14f72a2d1b2a1bbe86c2a8bc375661bfd5c30b5e8cee7c2fad9a44/streamlit-0.82.0-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 7.5MB/s \n",
            "\u001b[?25hCollecting blinker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 52.4MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/da/6f6224fdfc47dab57881fe20c0d1bc3122be290198ba0bf26a953a045d92/GitPython-3.1.17-py3-none-any.whl (166kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 49.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Collecting click<8.0,>=7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.2MB/s \n",
            "\u001b[?25hCollecting watchdog; platform_system != \"Darwin\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/5b/36b3b11e557830de6fc1dc06e9aa3ee274119b8cea9cc98175dbbf72cf87/watchdog-2.1.2-py3-none-manylinux2014_x86_64.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.2MB/s \n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tzlocal->streamlit) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (56.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (3.7.4.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.0MB/s \n",
            "\u001b[?25hCollecting ipykernel>=5.1.2; python_version >= \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/6d/6c8fe4b658f77947d4244ce81f60230c4c8d1dc1a21ae83e63b269339178/ipykernel-5.5.5-py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 57.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.0.3)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.9.5)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13448 sha256=655ce32c71feb4adaef985fd0d8d744495347687291b9216f47f78d426cad83c\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
            "Successfully built blinker\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: blinker, base58, smmap, gitdb, gitpython, click, watchdog, ipykernel, pydeck, validators, streamlit\n",
            "  Found existing installation: click 8.0.0\n",
            "    Uninstalling click-8.0.0:\n",
            "      Successfully uninstalled click-8.0.0\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "Successfully installed base58-2.1.0 blinker-1.4 click-7.1.2 gitdb-4.0.7 gitpython-3.1.17 ipykernel-5.5.5 pydeck-0.6.2 smmap-4.0.0 streamlit-0.82.0 validators-0.18.2 watchdog-2.1.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipykernel"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8uMIymrSMK9",
        "outputId": "f4f9d4da-fb3e-4c27-e6cf-663d0c935214"
      },
      "source": [
        "!pip install newspaper3k   #### newspapaper 3k for google search web scrapping "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 8.7MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.3MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 47.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp37-none-any.whl size=13538 sha256=0077663a032ee6bc663d68ce37253242c1b53aa61f8ea06bc08b46f76f6542b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp37-none-any.whl size=3358 sha256=99f5d5f6e309a62e248c4327defd7059fb6e4936a487d68d0af347d79b27266b\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp37-none-any.whl size=7398406 sha256=035e7769c682657df4c225b873708812216b5dca7215a020a13e9e39f8ca7199\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=83b8a20d61ae37daba4726d68f066536db83f8d2fa12ea5d2b63c90981272b04\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, cssselect, tinysegmenter, feedfinder2, requests-file, tldextract, jieba3k, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmc--sURSS_g",
        "outputId": "57feee2f-fdcd-4154-8bd1-1826da6e717c"
      },
      "source": [
        "!pip install transformers==4.2.0    #### for deeplearning model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/ea/634945faff8ad6984b98f7f3d98f6d83083a18af44e349744d90bde81f80/transformers-4.2.0-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 34.9MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.0) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.0) (4.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.0) (1.19.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.0) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.0) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.0) (3.4.1)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.9.4 transformers-4.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XfXQ63CSWau",
        "outputId": "b513d9ba-6deb-49fa-c028-092eddbc20db"
      },
      "source": [
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting simpletransformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/62/c27d9781c7469d4abe7a6e6658120bfb4a41535e8212d11f9d41d379af5d/simpletransformers-0.61.5-py3-none-any.whl (220kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 8.5MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/5f/45439b4767334b868e1c8c35b1b0ba3747d8c21be77b79f09eed7aa3c72b/wandb-0.10.30-py2.py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 28.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.19.5)\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.82.0)\n",
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.9.4)\n",
            "Requirement already satisfied: transformers>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (4.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers) (1.1.5)\n",
            "Collecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 23.7MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.47.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (2.8.1)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (2.3)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/4a/a54b254f67d8f4052338d54ebe90126f200693440a93ef76d254d581e3ec/sentry_sdk-1.1.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.4MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.3MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.12.4)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (3.1.17)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->simpletransformers) (7.1.2)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 45.6MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 53.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (20.9)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (4.0.1)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers) (0.70.11.1)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.18.2)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.6.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.1.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (5.1.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.0)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (2.1.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers) (1.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers) (2020.12.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb->simpletransformers) (56.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb->simpletransformers) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb->simpletransformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets->simpletransformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets->simpletransformers) (3.4.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.5)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (2.11.3)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->simpletransformers) (4.0.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.1.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.0.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->simpletransformers) (22.0.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.5)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.3.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n",
            "Building wheels for collected packages: seqeval, subprocess32, pathtools\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=48bfb07829fe0405c56555e30d2b775f510a01cd111590bbc4340cfcecdcf462\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=e0d53d627a5a82d8be25c374f060164ca8da0db3844b965a6699f7300ec416e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=6227500f5320ad3295c1ffda3586bd109a1927e2c16005dd59da1ce20ccede89\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built seqeval subprocess32 pathtools\n",
            "\u001b[31mERROR: datasets 1.6.2 has requirement tqdm<4.50.0,>=4.27, but you'll have tqdm 4.60.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: configparser, sentry-sdk, subprocess32, docker-pycreds, pathtools, shortuuid, wandb, tqdm, huggingface-hub, xxhash, fsspec, datasets, seqeval, tensorboardx, sentencepiece, simpletransformers\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed configparser-5.0.2 datasets-1.6.2 docker-pycreds-0.4.0 fsspec-2021.5.0 huggingface-hub-0.0.8 pathtools-0.1.2 sentencepiece-0.1.95 sentry-sdk-1.1.0 seqeval-1.2.2 shortuuid-1.0.1 simpletransformers-0.61.5 subprocess32-3.5.4 tensorboardx-2.2 tqdm-4.60.0 wandb-0.10.30 xxhash-2.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2imr8OgSYWk",
        "outputId": "d3a2a075-e18d-4cf7-936a-f7995e53a550"
      },
      "source": [
        "!pip install spacy_transformers   ### for deeplearning model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy_transformers\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/c5/a156f9c979cc14f5f41cf2e6ecfc55d1128ac0363930ec7cc6fe4d98b4a2/spacy_transformers-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: transformers<4.6.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy_transformers) (4.2.0)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 13.2MB/s \n",
            "\u001b[?25hCollecting spacy<3.1.0,>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 242kB/s \n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/86/a6786d24d1d8f3a6cff2c60b55a7e845725a94919cd94d270ea49d82e59b/spacy_alignments-0.8.3-cp37-cp37m-manylinux2014_x86_64.whl (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 49.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from spacy_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (20.9)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (4.60.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<4.6.0,>=3.4.0->spacy_transformers) (4.0.1)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/fa/d43f31874e1f2a9633e4c025be310f2ce7a8350017579e9e837a62630a7e/pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 144kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (56.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (2.11.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (2.0.5)\n",
            "Collecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (3.7.4.3)\n",
            "Collecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (1.0.5)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->spacy_transformers) (3.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.6.0,>=3.4.0->spacy_transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.6.0,>=3.4.0->spacy_transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.6.0,>=3.4.0->spacy_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.6.0,>=3.4.0->spacy_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.6.0,>=3.4.0->spacy_transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy_transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy_transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.6.0,>=3.4.0->spacy_transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<4.6.0,>=3.4.0->spacy_transformers) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->spacy_transformers) (2.0.0)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 54.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=617390f22e97184fad1216e9ac521f48d61871a5dc3aa85ebb6a31679730606f\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: catalogue, srsly, pydantic, typer, spacy-legacy, smart-open, pathy, thinc, spacy, spacy-alignments, spacy-transformers\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.4 smart-open-3.0.0 spacy-3.0.6 spacy-alignments-0.8.3 spacy-legacy-3.0.5 spacy-transformers-1.0.2 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g_3gC3VSbRV",
        "outputId": "3f50f5b9-7431-4359-9ed1-ff9e6bffb068"
      },
      "source": [
        "!pip install spacy  ### for NLP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.60.0)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.7.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.5.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.0)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (3.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmnUM9kFS9Gp",
        "outputId": "7f6219cc-ab27-47a7-aae7-ccb43f0b014a"
      },
      "source": [
        "!npm install localtunnel   ### For deployment "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.1\n",
            "added 22 packages from 22 contributors and audited 22 packages in 1.397s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z79bD_eb-nF-",
        "outputId": "4fdc14b9-ba65-47c8-d995-e561ebc3dfc6"
      },
      "source": [
        "!pip install streamlit --upgrade    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: streamlit in /usr/local/lib/python3.7/dist-packages (0.82.0)\n",
            "Requirement already satisfied, skipping upgrade: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied, skipping upgrade: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.18.2)\n",
            "Requirement already satisfied, skipping upgrade: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.0)\n",
            "Requirement already satisfied, skipping upgrade: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied, skipping upgrade: gitpython in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.17)\n",
            "Requirement already satisfied, skipping upgrade: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.2)\n",
            "Requirement already satisfied, skipping upgrade: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.6.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->streamlit) (4.0.7)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.5.5)\n",
            "Requirement already satisfied, skipping upgrade: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->streamlit) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.0.3)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied, skipping upgrade: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHekeEka-vbT",
        "outputId": "1cc0abda-460c-4322-addc-a8d637a73324"
      },
      "source": [
        "!npm install streamlit-component-lib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ streamlit-component-lib@1.2.0\n",
            "added 41 packages from 70 contributors and audited 77 packages in 2.705s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "\u001b[K\u001b[?25h\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[33m   ╭────────────────────────────────────────────────────────────────╮\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                                                \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m      New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m6.14.8\u001b[39m → \u001b[32m7.13.0\u001b[39m       \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m   \u001b[33mChangelog:\u001b[39m \u001b[36mhttps://github.com/npm/cli/releases/tag/v7.13.0\u001b[39m   \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m               Run \u001b[32mnpm install -g npm\u001b[39m to update!                \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                                                \u001b[33m│\u001b[39m\n",
            "\u001b[33m   ╰────────────────────────────────────────────────────────────────╯\u001b[39m\n",
            "\u001b[33m\u001b[39m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8FxnPi3SizO"
      },
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "import streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xqeLg7Y3tfc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf2-KrSL3uBu",
        "outputId": "03ef3622-9c89-4d6c-c1d3-6b8890736b6c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nLm0XBUL3qDk",
        "outputId": "48383584-8f65-43e8-ba3e-f66d84c7b8e0"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKnSvRs_2GcI",
        "outputId": "ed12d316-c2b8-40c8-b4b0-34af8fe1291b"
      },
      "source": [
        "%%writefile style.css\n",
        "\n",
        "body {\n",
        "    color: green;\n",
        "    background-color: #a6dfe0;\n",
        "    align-content: center;\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting style.css\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I965-assNKbo"
      },
      "source": [
        "## Installing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXwIV5LMNKbp"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from urllib.parse import urlparse\n",
        "from googlesearch import search\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "import itertools\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "import streamlit\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer# Text Cleaning And Preprocessing\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import transformers\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "from collections import defaultdict\n",
        "#from data.scorer import score_submission, print_confusion_matrix, score_defaults, SCORE_REPORT\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import os\n",
        "# import re\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import re\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from nltk.corpus import stopwords\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmwU_2aINKbq"
      },
      "source": [
        "## Webscrapping Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da3Ip6c2NKbq"
      },
      "source": [
        "our_url = pd.read_csv(\"Indian Newspapers - Sheet1.csv\")\n",
        "\n",
        "len(our_url)\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "our =our_url[\"URL\"].dropna()\n",
        "\n",
        "our = pd.DataFrame(our)\n",
        "\n",
        "def urls_domain(urls):\n",
        "    url_domain = []\n",
        "    for i in urls:\n",
        "        domain = urlparse(i).netloc\n",
        "        url_domain.append(domain)\n",
        "    return url_domain\n",
        "\n",
        "our_domain = urls_domain(urls = list(our[\"URL\"]))\n",
        "\n",
        "#list(our[\"URL\"])\n",
        "\n",
        "def web_scrapper_url_list(claim_list):\n",
        "    print(\"enter your search here \")\n",
        "    try:\n",
        "        from googlesearch import search\n",
        "    except ImportError:\n",
        "        print(\"No module named 'google' found\")\n",
        "\n",
        "# to search\n",
        "    list1 = []\n",
        "    for query in claim_list:\n",
        "        print(query)\n",
        "        q = claim_list.index(query)\n",
        "        for j in search(query, tld=\"co.in\", num=50 ,stop=50, pause=2):\n",
        "            #print(j)\n",
        "            time.sleep(5)\n",
        "            list1.append(j)\n",
        "    return list1\n",
        "\n",
        "\n",
        "####\n",
        "\n",
        "####\n",
        "def intersection(lst1, lst2):\n",
        "    lst3 = [value for value in lst1 if value in lst2]\n",
        "    return lst3\n",
        "\n",
        "\n",
        "def index(x ,url_domain ):\n",
        "    url_index = []\n",
        "    for i in x:\n",
        "        url_index.append(url_domain.index(i))\n",
        "    return url_index\n",
        "\n",
        "\n",
        "def finalurl(url_index ,urls):\n",
        "    final_url = []\n",
        "    for i in url_index:\n",
        "        final_url.append(urls[i])\n",
        "    return final_url\n",
        "\n",
        "####\n",
        "def web_scrapper_claim_list(claim_list):\n",
        "    c = []\n",
        "    for i in claim_list:\n",
        "        cl = list(itertools.repeat(i, 50))\n",
        "        c.append(cl)\n",
        "        \n",
        "    merged = list(itertools.chain.from_iterable(c))\n",
        "    return merged\n",
        "\n",
        "def web_scrapper_dataframe_function(xy):\n",
        "    article_title = []\n",
        "    article_text = []\n",
        "    article_summary = []\n",
        "    article_keywords = []\n",
        "    headline = []\n",
        "    domain = []\n",
        "    date = []\n",
        "    url_list1 = web_scrapper_url_list(claim_list = xy)\n",
        "    domain_url = urls_domain(urls = url_list1)\n",
        "    final_domain = intersection(lst1 = domain_url, lst2 = our_domain )\n",
        "    idx = index(x = final_domain , url_domain = domain_url)\n",
        "    url_list = finalurl(url_index = idx ,urls = url_list1)\n",
        "    domain_1 = urls_domain(urls = url_list)\n",
        "    URL = []\n",
        "    \n",
        "    headline_list = web_scrapper_claim_list(claim_list = xy)\n",
        "    for url in url_list:\n",
        "        try:\n",
        "            article = Article(url, language=\"en\") # en for English \n",
        "            article.download() \n",
        "            article.parse()\n",
        "            article.nlp() \n",
        "            time.sleep(1)\n",
        "            #print(\"Article Title:\") \n",
        "            #print(article.title) #prints the title of the article\n",
        "            title = article_title.append(article.title)\n",
        "\n",
        "        \n",
        "           \n",
        "            #print(\"Article Text:\") \n",
        "            #print(article.text) #prints the entire text of the article\n",
        "            text = article_text.append(article.text)\n",
        "            #print(\"\\n\") \n",
        "            #print(\"Article Summary:\") \n",
        "            #print(article.summary) #prints the summary of the article\n",
        "            summary = article_summary.append(article.summary)\n",
        "            \n",
        "            #print(\"\\n\") \n",
        "            #print(\"Article Keywords:\")\n",
        "            #print(article.keywords) #prints the keywords of the article\n",
        "            keywords = article_keywords.append(article.keywords)\n",
        "            p = url_list.index(url)\n",
        "            head = headline_list[p]\n",
        "            headline.append(head)\n",
        "            domain.append(domain_1[p])\n",
        "            URL.append(url)\n",
        "            time.sleep(4)\n",
        "        except:\n",
        "            print(\"loop got error \")\n",
        "\n",
        "\n",
        "    \n",
        "    df1 = {'title': article_title ,'text': article_text , 'summary': article_summary , 'keywords' : article_keywords , 'headline':headline , \"Newspaper\":domain , \"URL\":URL}\n",
        "    df = pd.DataFrame(df1)\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSAvWxEyNKbs"
      },
      "source": [
        "### Function to clean text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wptB3QkNKbs"
      },
      "source": [
        "def clean_text(df):\n",
        "    all_reviews = list()\n",
        "    \n",
        "    # Taking all the text from the Data Frame\n",
        "    lines = df.values.tolist()\n",
        "    \n",
        "    #Iterating Through All the Documents\n",
        "    for text in lines:\n",
        "        text = str(text).lower()          # Convert into Lower case\n",
        "        \n",
        "        \n",
        "        #Removing all the URL Links from the Corpus and replace them with empty or common string\n",
        "        pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "        text = pattern.sub('', text)\n",
        "        \n",
        "        #Removing the Emojis and replacing them from empty or common string\n",
        "        emoji = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        text = emoji.sub(r'', text)\n",
        "        \n",
        "        \n",
        "        # Expanding Common Short Forms\n",
        "        # Not very Helpful in Sentimental Analysis\n",
        "        # These are pronoun and verbs.\n",
        "        # Important if you want to create a text from your model along with Classifying the sentinment.\n",
        "        # Helpful when you are creating Chatbot.\n",
        "        \n",
        "        text = re.sub(r\"i'm\", \"i am\", text)\n",
        "        text = re.sub(r\"he's\", \"he is\", text)\n",
        "        text = re.sub(r\"she's\", \"she is\", text)\n",
        "        text = re.sub(r\"that's\", \"that is\", text)        \n",
        "        text = re.sub(r\"what's\", \"what is\", text)\n",
        "        text = re.sub(r\"where's\", \"where is\", text) \n",
        "        text = re.sub(r\"\\'ll\", \" will\", text)  \n",
        "        text = re.sub(r\"\\'ve\", \" have\", text)  \n",
        "        text = re.sub(r\"\\'re\", \" are\", text)\n",
        "        text = re.sub(r\"\\'d\", \" would\", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "        text = re.sub(r\"won't\", \"will not\", text)\n",
        "        text = re.sub(r\"don't\", \"do not\", text)\n",
        "        text = re.sub(r\"did't\", \"did not\", text)\n",
        "        text = re.sub(r\"can't\", \"can not\", text)\n",
        "        text = re.sub(r\"it's\", \"it is\", text)\n",
        "        text = re.sub(r\"couldn't\", \"could not\", text)\n",
        "        text = re.sub(r\"have't\", \"have not\", text)\n",
        "        \n",
        "        # Removing All the Punctuation\n",
        "        text = re.sub(r\"[,.\\\"!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
        "        \n",
        "        # Tokenize the Sentences(Split the sentences by Spaces)\n",
        "        #(Can also Use Split Function), But Tokenize function returns the output as a list\n",
        "        tokens = word_tokenize(text)\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        \n",
        "        # Removing Everything which is not a Alphabet\n",
        "        stripped = [w.translate(table) for w in tokens]\n",
        "        \n",
        "         # Removing The Stop words\n",
        "        words = [word for word in stripped if word.isalpha()]\n",
        "#         stop_words = set(stopwords.words(\"english\"))\n",
        "#         stop_words.discard(\"not\")    # Discarding Not from stopwords (can be important or  sentiment analysis)\n",
        "#         words = [w for w in words if not w in stop_words]\n",
        "\n",
        "\n",
        "        words = ' '.join(words)     # Joining the Token\n",
        "        all_reviews.append(words)     # Adding into Large List.\n",
        "    return all_reviews\n",
        "\n",
        "\n",
        "def process (df, var):\n",
        "    \n",
        "    for i in var:\n",
        "        df[i] = clean_text(df[i])\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIrKzOt5NKbu"
      },
      "source": [
        "### Other function to be used in pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT8XD-vhNKbu"
      },
      "source": [
        "# Creating A Function to calculate the Jaccard Similarity\n",
        "\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return len(s1.intersection(s2)) / len(s1.union(s2))\n",
        "\n",
        "glove_vectors = {}\n",
        "\n",
        "def add_jaccard_similarity(data):\n",
        "    count=0\n",
        "    for i in range(data.shape[0]):\n",
        "        jaccard_lis=[];eps=0.001\n",
        "        sentence = data.loc[i,'articleBody'].split('.') #per sentence scorer\n",
        "        for j in range(len(sentence)):\n",
        "            jaccard_lis.append(jaccard_similarity(data.loc[i,'Headline'].split(' '),sentence[j].split(' ')))\n",
        "        max_jaccard_similarity = max(jaccard_lis)\n",
        "        avg_jaccard_similarity = sum(jaccard_lis)/len(jaccard_lis)\n",
        "        min_jaccard_similarity = min(jaccard_lis)\n",
        "        data.loc[i,'jaccard_similarity'] = (max_jaccard_similarity+min_jaccard_similarity)/(max_jaccard_similarity-min_jaccard_similarity+eps)\n",
        "        if i%1000==0:\n",
        "            count+=1\n",
        "            print(\"Processed {0} Headlines\".format(count*1000))\n",
        "            \n",
        "            \n",
        "# Calculating Jaccrd Similarity for Two Columns With Only Nouns\n",
        "\n",
        "def add_jaccard_similarity_noun(data):\n",
        "    count=0\n",
        "    for i in range(data.shape[0]):\n",
        "        jaccard_lis=[];eps=0.001\n",
        "        sentence = data.loc[i,'articleBody_Nouns'].split('.') #per sentence scorer\n",
        "        for j in range(len(sentence)):\n",
        "            jaccard_lis.append(jaccard_similarity(data.loc[i,'Headline_Nouns'].split(' '),sentence[j].split(' ')))\n",
        "        max_jaccard_similarity = max(jaccard_lis)\n",
        "        avg_jaccard_similarity = sum(jaccard_lis)/len(jaccard_lis)\n",
        "        min_jaccard_similarity = min(jaccard_lis)\n",
        "        data.loc[i,'jaccard_similarity_nouns'] = (max_jaccard_similarity+min_jaccard_similarity)/(max_jaccard_similarity-min_jaccard_similarity+eps)\n",
        "        if i%1000==0:\n",
        "            count+=1\n",
        "            print(\"Processed {0} Headlines_nouns\".format(count*1000))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmaGkYDPNKbv"
      },
      "source": [
        "## Pipeline 1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i86moa_NKbw"
      },
      "source": [
        "def pipeline1(your_claim):\n",
        "    Train_data =  web_scrapper_dataframe_function(xy = your_claim)\n",
        "    raw_data = Train_data.copy()\n",
        "\n",
        "    Train_data = Train_data.rename(columns = {'headline': 'Headline', 'text': 'articleBody'}, inplace = False)\n",
        "    Train_data = Train_data[['Headline', 'articleBody']]\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"your claim got\", len(Train_data) ,\" results out of 162 major newspaper sites \")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    ### CLEANING TEXT\n",
        "    Train_data1 = process(Train_data, [\"Headline\", \"articleBody\"])\n",
        "    Train_data1 = Train_data1.dropna()\n",
        "    #print(\"checking null values\")\n",
        "    #print(Train_data1.isnull().sum())\n",
        "    \n",
        "    \n",
        "    ### removing stop words \n",
        "    \n",
        "    stopwords_english = set(stopwords.words('english'))\n",
        "    Train_data1['Headline'] = Train_data1.Headline.apply(lambda x:str(x))\n",
        "    Train_data1.loc[:,'Headline'] = Train_data1['Headline'].apply(lambda x : str.lower(x))\n",
        "    Train_data1.loc[:,'Headline'] = Train_data1['Headline'].apply(lambda x:' '.join(re.findall('[\\w]+',x)))\n",
        "    Train_data1.loc[:,'articleBody'] = Train_data1['articleBody'].apply(lambda x : str.lower(x))\n",
        "    Train_data1.loc[:,'articleBody'] = Train_data1['articleBody'].apply(lambda x:' '.join(re.findall('[\\w]+',x)))\n",
        "\n",
        "\n",
        "    def remove_stopwords(s):\n",
        "        return ' '.join(word for word in s.split() if word not in stopwords_english)\n",
        "\n",
        "    Train_data1['Headline'] = Train_data1['Headline'].apply(lambda x:remove_stopwords(x))\n",
        "    Train_data1['articleBody'] = Train_data1['articleBody'].apply(lambda x:str(x))\n",
        "    Train_data1['articleBody'] = Train_data1['articleBody'].apply(lambda x:remove_stopwords(x))\n",
        "\n",
        "    Train_data2 = Train_data1.copy()\n",
        "    def noun_extraction(df, col):\n",
        "        l1_nouns = []\n",
        "        l1  = df[col]\n",
        "        for l in l1:\n",
        "            tokn1 = nltk.word_tokenize(l)\n",
        "            nouns1 = [word for (word, pos) in nltk.pos_tag(tokn1) if(pos[:2] == 'NN')]\n",
        "            l1_nouns.append(nouns1)\n",
        "\n",
        "        l1_nouns1 = []\n",
        "        for i in l1_nouns:\n",
        "            l1_nouns1.append(\" \".join(set(i)))\n",
        "\n",
        "        df[col + \"_Nouns\"] = l1_nouns1\n",
        "        \n",
        "    \n",
        "    noun_extraction(Train_data2, \"articleBody\")\n",
        "    noun_extraction(Train_data2, \"Headline\")\n",
        "    add_jaccard_similarity_noun(Train_data2)\n",
        "    \n",
        "\n",
        "    return Train_data ,Train_data2 , raw_data\n",
        "\n",
        "#xyz = pipeline1(your_claim = [\"190 countries have pre-booked the Bharat Biotech COVID-19 vaccine that India has developed\"])\n",
        "\n",
        "#xyz\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32s1GM2KNKbx"
      },
      "source": [
        "### Pipeline 2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IXCB0AyNKbx"
      },
      "source": [
        "def pipeline2_feature_extraction(data):\n",
        "    Train_data2 = data.copy()\n",
        "    pattern = re.compile(\"[^a-zA-Z0-9 ]+\")  # strip punctuation, symbols, etc.\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    def tokenise(text):\n",
        "        text = pattern.sub('', str(text).replace('\\n', ' ').replace('-', ' ').lower())\n",
        "        text = [word for word in word_tokenize(text) if word not in stop_words]\n",
        "        return text\n",
        "\n",
        "\n",
        "    tokenize_list = []\n",
        "    for i in Train_data2[\"Headline\"]:\n",
        "        tokenize_list.append(tokenise(i))\n",
        "\n",
        "    tokenize_list_article = []\n",
        "    for i in Train_data2[\"articleBody\"]:\n",
        "        tokenize_list_article.append(tokenise(i))\n",
        "\n",
        "    Train_data2[\"articleBody\"] = tokenize_list_article\n",
        "    Train_data2[\"Headline\"] = tokenize_list\n",
        "\n",
        "\n",
        "    Train_data2[\"Headline\"] = Train_data2[\"Headline\"].str.join(', ')\n",
        "    Train_data2[\"articleBody\"] = Train_data2[\"articleBody\"].str.join(', ')\n",
        "\n",
        "    t_array = np.array(Train_data2)\n",
        "    corpus = np.r_[t_array[:, 1], t_array[:, 0]]  # 0 to 44973 are bodies, 44974 to 89943 are headlines\n",
        "\n",
        "\n",
        "    tokenize_list = []\n",
        "    for i in Train_data2[\"Headline\"]:\n",
        "        tokenize_list.append(tokenise(i))\n",
        "\n",
        "    tokenize_list_article = []\n",
        "    for i in Train_data2[\"articleBody\"]:\n",
        "        tokenize_list_article.append(tokenise(i))\n",
        "\n",
        "\n",
        "    Train_data2[\"articleBody\"] = tokenize_list_article\n",
        "    Train_data2[\"Headline\"] = tokenize_list\n",
        "\n",
        "\n",
        "    Train_data2[\"Headline\"] = Train_data2[\"Headline\"].str.join(', ')\n",
        "    Train_data2[\"articleBody\"] = Train_data2[\"articleBody\"].str.join(', ')\n",
        "\n",
        "    # Compute term-frequency of words in documents\n",
        "    def doc_to_tf(text, ngram=1):\n",
        "        words = tokenise(text)\n",
        "        ret = defaultdict(float)\n",
        "        for i in range(len(words)):\n",
        "            for j in range(1, ngram+1):\n",
        "                if i - j < 0:\n",
        "                    break\n",
        "                word = [words[i-k] for k in range(j)]\n",
        "                ret[word[0] if ngram == 1 else tuple(word)] += 1.0\n",
        "        return ret\n",
        "\n",
        "\n",
        "\n",
        "    basket = []\n",
        "    for i in range(len(t_array)):\n",
        "        #print(doc_to_tf(text = t_array[i,0]))\n",
        "        basket.append(doc_to_tf(text = t_array[i,0]))\n",
        "\n",
        "\n",
        "    corpus = np.r_[t_array[:, 1], t_array[:, 0]] \n",
        "\n",
        "    \n",
        "    \n",
        "    # Learn idf of every word in the corpus\n",
        "    df = defaultdict(float)\n",
        "    for doc in tqdm(corpus):\n",
        "        words = tokenise(doc)\n",
        "        seen = set()\n",
        "        for word in words:\n",
        "            if word not in seen:\n",
        "                df[word] += 1.0\n",
        "                seen.add(word)\n",
        "\n",
        "    num_docs = corpus.shape[0]\n",
        "    idf = defaultdict(float)\n",
        "    for word, val in tqdm(df.items()):\n",
        "        idf[word] = np.log((1.0 + num_docs) / (1.0 + val)) + 1.0  # smoothed idf\n",
        "\n",
        "\n",
        "    f_glove = open('glove.6B.50d.txt', \"rb\")  # download from https://nlp.stanford.edu/projects/glove/\n",
        "    glove_vectors = {}\n",
        "    for line in tqdm(f_glove):\n",
        "        glove_vectors[str(line.split()[0]).split(\"'\")[1]] = np.array(list(map(float, line.split()[1:])))\n",
        "\n",
        "    #print(glove_vectors['glove'])\n",
        "    \n",
        "    \n",
        "    def doc_to_glove(doc):\n",
        "        doc_tf = doc_to_tf(doc)\n",
        "        doc_tf_idf = defaultdict(float)\n",
        "        for word, tf in doc_tf.items():\n",
        "            doc_tf_idf[word] = tf * idf[word]\n",
        "\n",
        "        doc_vector = np.zeros(glove_vectors['glove'].shape[0])\n",
        "        if np.sum(list(doc_tf_idf.values())) == 0.0:  # edge case: document is empty\n",
        "            return doc_vector\n",
        "\n",
        "        for word, tf_idf in doc_tf_idf.items():\n",
        "            if word in glove_vectors:\n",
        "                doc_vector += glove_vectors[word] * tf_idf\n",
        "        doc_vector /= np.sum(list(doc_tf_idf.values()))\n",
        "        return doc_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    glove_headline = []\n",
        "    for i in range(len(t_array)):\n",
        "        glove_headline.append(doc_to_glove(t_array[i, 0]))\n",
        "\n",
        "\n",
        "    # Compute term-frequency of words in documents\n",
        "    def doc_to_tf(text, ngram=1):\n",
        "        words = tokenise(text)\n",
        "        ret = defaultdict(float)\n",
        "        for i in range(len(words)):\n",
        "            for j in range(1, ngram+1):\n",
        "                if i - j < 0:\n",
        "                    break\n",
        "                word = [words[i-k] for k in range(j)]\n",
        "                ret[word[0] if ngram == 1 else tuple(word)] += 1.0\n",
        "        return ret\n",
        "\n",
        "    basket_article = []\n",
        "    for i in range(len(t_array)):\n",
        "        #print(doc_to_tf(text = t_array[i,0]))\n",
        "        basket_article.append(doc_to_tf(text = t_array[i,1]))\n",
        "\n",
        "\n",
        "    corpus = np.r_[t_array[:, 1], t_array[:, 0]]  \n",
        "\n",
        "    df = defaultdict(float)\n",
        "    for doc in tqdm(corpus):\n",
        "        words = tokenise(doc)\n",
        "        seen = set()\n",
        "        for word in words:\n",
        "            if word not in seen:\n",
        "                df[word] += 1.0\n",
        "                seen.add(word)\n",
        "\n",
        "    num_docs = corpus.shape[0]\n",
        "    idf = defaultdict(float)\n",
        "    for word, val in tqdm(df.items()):\n",
        "        idf[word] = np.log((1.0 + num_docs) / (1.0 + val)) + 1.0  # smoothed idf\n",
        "\n",
        "\n",
        "\n",
        "    f_glove = open('glove.6B.50d.txt', \"rb\")  # download from https://nlp.stanford.edu/projects/glove/\n",
        "    glove_vectors = {}\n",
        "    for line in tqdm(f_glove):\n",
        "        glove_vectors[str(line.split()[0]).split(\"'\")[1]] = np.array(list(map(float, line.split()[1:])))\n",
        "\n",
        "    #print(glove_vectors['glove'])\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "    def doc_to_glove(doc):\n",
        "        doc_tf = doc_to_tf(doc)\n",
        "        doc_tf_idf = defaultdict(float)\n",
        "        for word, tf in doc_tf.items():\n",
        "            doc_tf_idf[word] = tf * idf[word]\n",
        "\n",
        "        doc_vector = np.zeros(glove_vectors['glove'].shape[0])\n",
        "        if np.sum(list(doc_tf_idf.values())) == 0.0:  # edge case: document is empty\n",
        "            return doc_vector\n",
        "\n",
        "        for word, tf_idf in doc_tf_idf.items():\n",
        "            if word in glove_vectors:\n",
        "                doc_vector += glove_vectors[word] * tf_idf\n",
        "        doc_vector /= np.sum(list(doc_tf_idf.values()))\n",
        "        return doc_vector\n",
        "\n",
        "    glove_article = []\n",
        "    for i in range(len(t_array)):\n",
        "        glove_article.append(doc_to_glove(t_array[i, 1]))\n",
        "\n",
        "\n",
        "    # Compute cosine similarity of GLoVe vectors for all headline-body pairs\n",
        "    def dot_product(vec1, vec2):\n",
        "        sigma = 0.0\n",
        "        for i in range(vec1.shape[0]):  # assume vec1 and vec2 has same shape\n",
        "            sigma += vec1[i] * vec2[i]\n",
        "        return sigma\n",
        "\n",
        "    def magnitude(vec):\n",
        "        return np.sqrt(np.sum(np.square(vec)))\n",
        "\n",
        "\n",
        "    def cosine_similarity(head , body):\n",
        "        headline_vector = head\n",
        "        body_vector = body\n",
        "\n",
        "        if magnitude(headline_vector) == 0.0 or magnitude(body_vector) == 0.0:  # edge case: document is empty\n",
        "            return 0.0\n",
        "\n",
        "        return dot_product(headline_vector, body_vector) / (magnitude(headline_vector) * magnitude(body_vector))\n",
        "\n",
        "\n",
        "    glove_similarity = []\n",
        "    for i in range(len(glove_headline)):\n",
        "        glove_similarity.append(cosine_similarity(head = glove_headline[i] ,body =  glove_article[i]))\n",
        "\n",
        "    Train_data2['glove_similarity'] = glove_similarity\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    def divergence(lm1, lm2):\n",
        "        sigma = 0.0\n",
        "        for i in range(lm1.shape[0]):  # assume lm1 and lm2 has same shape\n",
        "            sigma += lm1[i] * np.log(lm1[i] / lm2[i])\n",
        "        return sigma\n",
        "\n",
        "\n",
        "    def kl_divergence(head , article, eps=0.1):\n",
        "        # Convert headline and body to 1-gram representations\n",
        "        tf_headline = head\n",
        "        tf_body = article\n",
        "        # Convert dictionary tf representations to vectors (make sure columns match to the same word)\n",
        "        words = set(tf_headline.keys()).union(set(tf_body.keys()))\n",
        "        vec_headline, vec_body = np.zeros(len(words)), np.zeros(len(words))\n",
        "        i = 0\n",
        "        for word in words:\n",
        "            vec_headline[i] += tf_headline[word]\n",
        "            vec_body[i] = tf_body[word]\n",
        "            i += 1\n",
        "\n",
        "        # Compute a simple 1-gram language model of headline and body\n",
        "        lm_headline = vec_headline + eps\n",
        "        lm_headline /= np.sum(lm_headline)\n",
        "        lm_body = vec_body + eps\n",
        "        lm_body /= np.sum(lm_body)\n",
        "\n",
        "        # Return KL-divergence of both language models\n",
        "        return divergence(lm_headline, lm_body)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kl_diverge_list = []\n",
        "    for i in range(len(t_array)):\n",
        "        kl_diverge_list.append(kl_divergence(head = basket[i], article = basket_article[i]))\n",
        "\n",
        "    Train_data2['kl_divergence'] = kl_diverge_list\n",
        "\n",
        "\n",
        "    # Other feature 1\n",
        "    def ngram_overlap(head , article):\n",
        "        # Returns how many times n-grams (up to 3-gram) that occur in the article's headline occur on the article's body.\n",
        "        tf_headline = doc_to_tf(head, ngram=3)\n",
        "        tf_body = doc_to_tf(article, ngram=3)\n",
        "        matches = 0.0\n",
        "        for words in tf_headline.keys():\n",
        "            if words in tf_body:\n",
        "                matches += tf_body[words]\n",
        "        return np.power((matches), 1 / np.e)  # normalise for document length\n",
        "\n",
        "\n",
        "\n",
        "    ngram_overlap_list = []\n",
        "    for i in range(len(t_array)):\n",
        "        ngram_overlap_list.append(ngram_overlap(head = t_array[i,0] , article = t_array[i,0]))\n",
        "\n",
        "    Train_data2['ngram_overlap'] = ngram_overlap_list\n",
        "    #print(Train_data2)\n",
        "    return Train_data2\n",
        "\n",
        "\n",
        "\n",
        "#xyz2 = pipeline2_feature_extraction(data = xyz)\n",
        "\n",
        "#xyz2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVcYmZSzNKbz"
      },
      "source": [
        "### Pipeline 3 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vMEC_zlNKb0"
      },
      "source": [
        "train = pd.read_csv(\"final_train_data.csv\")\n",
        "\n",
        "def pipeline3_final_prediction_baseline_model1(dataframe):\n",
        "    train = pd.read_csv(\"final_train_data.csv\")\n",
        "    test = dataframe.copy()\n",
        "    \n",
        "    \n",
        "    xtrain3 = train[['jaccard_similarity_nouns', 'glove_similarity', 'ngram_overlap']]\n",
        "    ytrain3 = train['stance_base'] \n",
        "    xtest3 = test[['jaccard_similarity_nouns', 'glove_similarity', 'ngram_overlap']]\n",
        "    #ytest3 = test[\"Stance\"]\n",
        "    rg = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
        "\n",
        "    \n",
        "    xtrain3 = xtrain3.values.reshape(-1,3)\n",
        "    xtest3 = xtest3.values.reshape(-1,3)\n",
        "    \n",
        "    \n",
        "    rg.fit(xtrain3,ytrain3)\n",
        "    ypred2 = rg.predict(xtest3)\n",
        "    \n",
        "    \n",
        "    test[\"predicted_stance_base\"] = ypred2\n",
        "    \n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"model 1 output \")\n",
        "    print(test)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    test_eda = test[[\"Headline\" ,\"articleBody\",\"predicted_stance_base\"]]\n",
        "    z = test_eda.groupby(['Headline','predicted_stance_base']).count()\n",
        "    z.rename(columns = {'articleBody': 'count'}, inplace = True)\n",
        "    print(\"model 1 value counts total related and unrelated \")\n",
        "    print(z)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    return test , z\n",
        "\n",
        "#xyz3 = pipeline3_final_prediction_baseline_model1(dataframe = Train_data2)\n",
        "\n",
        "#xyz3\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plj2cZ3nNKb0"
      },
      "source": [
        "## Pipleline 4\n",
        "## Bert Model `"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwBXMpCBS-PS",
        "outputId": "8b661808-02df-40e3-dab4-08c061f9b288"
      },
      "source": [
        "\n",
        "\n",
        "def bart_summarizer(original_text):\n",
        "    device='cuda'\n",
        "    bart_read=open('BART_summarizer.pkl','rb')\n",
        "    tokenizer_read=open('BART_tokenizer.pkl','rb')\n",
        "    bart_model=pickle.load(bart_read)\n",
        "    load_tokenizer=pickle.load(tokenizer_read)\n",
        "    # Encoding the inputs and passing them to model.generate()\n",
        "    inputs = load_tokenizer.batch_encode_plus([original_text],return_tensors='pt', truncation=True)\n",
        "    inputs=inputs.to(device)\n",
        "    summary_ids = bart_model.generate(inputs['input_ids'], early_stopping=True)\n",
        "    \n",
        "    # Decoding and printing the summary\n",
        "    bart_summary = load_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    \n",
        "    return bart_summary\n",
        "\n",
        "def create_bart_summarizer(df_scraped):                          # dataset should contain only Headline and articlebody\n",
        "     \n",
        "    df_clean=df_scraped[[\"Headline\",\"articleBody\"]].dropna(axis=0)\n",
        "    df_idx=df_clean.reset_index(drop=True)\n",
        "    df_idx=df_idx[[\"Headline\",\"articleBody\"]]\n",
        "    df_text=df_idx.rename(columns={'Headline':'text_a'})\n",
        "    \n",
        "    summ = []\n",
        "\n",
        "    for text in df_text[\"articleBody\"]:\n",
        "        \n",
        "        summ.append(bart_summarizer(str(text)))\n",
        "\n",
        "    summary_df=pd.DataFrame(summ,columns=['text_b'])\n",
        "    df_text['text_b']=summary_df\n",
        "    summarized_df=df_text[['text_a',\"text_b\"]]\n",
        "    return summarized_df\n",
        "\n",
        "def Bert_classification(summarized_df):\n",
        "    BERT_model=open('BERTmodel_3class.pkl','rb')\n",
        "    loaded_BERTmodel=pickle.load(BERT_model)\n",
        "    \n",
        "    #rf=pd.read_csv(rf_file,encoding= 'unicode_escape')\n",
        "    \n",
        "    #BERT_df=summarized_df[summarized_df['predicted_stance_base']=='related']\n",
        "    Bert_df=summarized_df.rename({'Headline':'text_a','articleBody_pos':'text_b'},axis=1) \n",
        "    Bert_test=Bert_df[['text_a','text_b']]\n",
        "    Bert_test=Bert_test.dropna()\n",
        "    predicted_labels_result=[]\n",
        "    for i in range(0,len(Bert_test)):\n",
        "          predictions,raw_outputs=loaded_BERTmodel.predict([[Bert_test['text_a'].iloc[i],Bert_test['text_b'].iloc[i]]])\n",
        "          predicted_labels_result.append(predictions)\n",
        "    \n",
        "    BERT_prediction=pd.DataFrame(predicted_labels_result)\n",
        "    Bert_test_indx=Bert_test.reset_index(drop=True)\n",
        "    Bert_test_indx['BERT_predicted_labels']=BERT_prediction\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(\"Bert model output \")\n",
        "    print(\" 0 = Agree , 1 Disagree , 2 Discuss \")\n",
        "    Bert_test_indx[\"stance\"] = Bert_test_indx['BERT_predicted_labels']\n",
        "    Bert_test_indx.replace({'stance':{0:'Agree', 1:'Disagree' ,2:'Discuss'}}, inplace=True)\n",
        "    print(Bert_test_indx)\n",
        "    test_eda_2 = Bert_test_indx[[\"text_a\" ,\"text_b\",\"BERT_predicted_labels\",\"stance\"]]\n",
        "    bert_result = test_eda_2.groupby(['text_a','stance']).count()\n",
        "    bert_result.rename(columns = {'text_b': 'stance_total_count'}, inplace = True)\n",
        "    print(bert_result)\n",
        "    return Bert_test_indx , bert_result\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting dehoxified.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYUXl1jINKb2"
      },
      "source": [
        "# Creating final function using/mergin all the pipeline function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E65zBZn3NKb2"
      },
      "source": [
        "def FINAL_PIPELINE(clm):\n",
        "   \n",
        "    summary_df , xyz , RAW_DATA = pipeline1(your_claim = clm )\n",
        "    xyz2 = pipeline2_feature_extraction(data = xyz)\n",
        "    TEST , xyz3 = pipeline3_final_prediction_baseline_model1(dataframe = xyz2)\n",
        "    summary_df['predicted_stance_base'] = TEST['predicted_stance_base']\n",
        "    summary_df_related =summary_df[summary_df['predicted_stance_base']=='related'] \n",
        "    summary_df_sorted = summary_df_related[[\"Headline\",\"articleBody\"]]  \n",
        "    bart_summ = create_bart_summarizer(df_scraped = summary_df_sorted)\n",
        "    final_bert ,BERT_RESULTS  = Bert_classification(summarized_df = bart_summ)\n",
        "\n",
        "    return RAW_DATA ,TEST ,  final_bert\n",
        "\n",
        "\n",
        "#### we are using this function to present our model\n",
        "def presentation_function(news):\n",
        "    a , b , c = FINAL_PIPELINE(clm = news)\n",
        "    a[\"predicted_stance_base\"] = b[\"predicted_stance_base\"]\n",
        "    a_related = pd.DataFrame(a[a.predicted_stance_base == \"related\"])\n",
        "    a_related['predicted_stance_base'] = list(c[\"stance\"])\n",
        "    a_unrelated = pd.DataFrame(a[a.predicted_stance_base != \"related\"])\n",
        "    final_frame = pd.concat([a_related , a_unrelated])\n",
        "    final_frame = final_frame.reset_index()\n",
        "    print(pd.DataFrame(final_frame['predicted_stance_base'].value_counts()))\n",
        "    print(final_frame)\n",
        "\n",
        "\n",
        "\n",
        "    output_df= pd.DataFrame({\n",
        "      \"Labels\" : [\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"],\n",
        "      \"Count\" : [final_frame[final_frame[\"predicted_stance_base\"] == \"Agree\"][\"predicted_stance_base\"].count(),\n",
        "                final_frame[final_frame[\"predicted_stance_base\"] == \"Disagree\"][\"predicted_stance_base\"].count(),\n",
        "                final_frame[final_frame[\"predicted_stance_base\"] == \"Discuss\"][\"predicted_stance_base\"].count(),\n",
        "                final_frame[final_frame[\"predicted_stance_base\"] == \"unrelated\"][\"predicted_stance_base\"].count()]\n",
        "    })\n",
        "\n",
        "    output_df\n",
        "\n",
        "    output_df = output_df[output_df.Count != 0]\n",
        "    streamlit.table(output_df)\n",
        "    mycolors = [\"Green\", \"red\", \"grey\", \"#FDFEEB\"]   # #FDFEEB: Ivory\n",
        "    fig, ax = plt.subplots(figsize=(2,2))\n",
        "    # fig.patch.set_facecolor('#1D3A45')\n",
        "    ax.pie(output_df[\"Count\"], labels=output_df[\"Labels\"], startangle=90, colors=mycolors)\n",
        "    plt.title(\"Distribution of Scapped Article Stance on the Claim\")\n",
        "    streamlit.pyplot(fig)\n",
        "    final_frame2 = final_frame[[\"Newspaper\",\"URL\",\"predicted_stance_base\"]]\n",
        "    print(final_frame2)\n",
        "    return final_frame2\n",
        "\n",
        "\n",
        "\n",
        "streamlit.title(\"DEHOXIFIED\")\n",
        "claim = streamlit.text_input('Please input your claim here' , )\n",
        "\n",
        "if claim:\n",
        "    pf = presentation_function(news = [claim] )\n",
        "    streamlit.table(pf)\n",
        "else:\n",
        "    print(\"invalid input\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZgyalBwNKb3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsPjmqpEUn5I"
      },
      "source": [
        "#os.chdir(\"/content/drive/MyDrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNE6hOcZUuO_",
        "outputId": "4dc42f16-f504-4332-9ba5-cfdf5a0c1179"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Adhar cards n pan'\t\t 'images and sign'\n",
            "'aptitude exam results'\t\t 'IMAGES AND SIGN'\n",
            " BART_summarizer.pkl\t\t 'IMP_Python and other files praxis'\n",
            " BART_tokenizer.pkl\t\t 'Indian Newspapers - Sheet1.csv'\n",
            " BERTmodel_3class.pkl\t\t  results\n",
            "'Claims (135).xlsx'\t\t  streamlit_test.py\n",
            "'Colab Notebooks'\t\t  style.css\n",
            " data_collection_real_11_20.csv  'temporary files'\n",
            " data_collection_real_2.csv\t 'Term 1_ Praxis'\n",
            " dehoxified.py\t\t\t  Term_2_praxis\n",
            " final_train_data.csv\t\t  test.py\n",
            " glove.6B.50d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nyKUQkGT17U"
      },
      "source": [
        "!streamlit run dehoxified.py &>/dev/null&"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_4zsFsDT69s",
        "outputId": "d4fd4238-3bb3-4411-9823-44d2b94a35d9"
      },
      "source": [
        "!npx localtunnel --port 8501"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.374s\n",
            "your url is: https://mean-mayfly-61.loca.lt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxP8THGT9my"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}